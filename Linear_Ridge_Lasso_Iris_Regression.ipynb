{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "630f2b5e",
   "metadata": {},
   "source": [
    "\n",
    "# Linear Regression, Ridge Regression, and Lasso Regression on Iris Dataset\n",
    "\n",
    "This notebook demonstrates the concepts of Linear Regression, Ridge Regression, and Lasso Regression using the Iris dataset. We will predict one feature (petal width) based on the other features and compare the models' performance.\n",
    "\n",
    "## Step 1: Import Libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cae1d9ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Import necessary libraries\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d462aa41",
   "metadata": {},
   "source": [
    "## Step 2: Load the Iris Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b350af3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal length (cm)</th>\n",
       "      <th>sepal width (cm)</th>\n",
       "      <th>petal length (cm)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sepal length (cm)  sepal width (cm)  petal length (cm)\n",
       "0                5.1               3.5                1.4\n",
       "1                4.9               3.0                1.4\n",
       "2                4.7               3.2                1.3\n",
       "3                4.6               3.1                1.5\n",
       "4                5.0               3.6                1.4"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Load the Iris dataset\n",
    "data = load_iris()\n",
    "X = pd.DataFrame(data.data, columns=data.feature_names)  # Features\n",
    "y = X[\"petal width (cm)\"]  # Target: Predict petal width\n",
    "X = X.drop(columns=[\"petal width (cm)\"])  # Drop target column from features\n",
    "X.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d94b463",
   "metadata": {},
   "source": [
    "## Step 3: Split the Dataset into Training and Testing Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6dc9786a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ddf4f68",
   "metadata": {},
   "source": [
    "## Step 4: Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8ebb19bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Regression MSE: 0.03771225865005799\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Linear Regression\n",
    "linear_model = LinearRegression()\n",
    "linear_model.fit(X_train, y_train)\n",
    "y_pred_linear = linear_model.predict(X_test)\n",
    "mse_linear = mean_squared_error(y_test, y_pred_linear)\n",
    "print(f\"Linear Regression MSE: {mse_linear}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b4c41fe",
   "metadata": {},
   "source": [
    "## Step 5: Ridge Regression (L2 Penalty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1a7c3542",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ridge Regression MSE: 0.03638154851286775\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Ridge Regression (with L2 penalty)\n",
    "ridge_model = Ridge(alpha=1.0)  # alpha is the lambda (regularization strength)\n",
    "ridge_model.fit(X_train, y_train)\n",
    "y_pred_ridge = ridge_model.predict(X_test)\n",
    "mse_ridge = mean_squared_error(y_test, y_pred_ridge)\n",
    "print(f\"Ridge Regression MSE: {mse_ridge}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a363f868",
   "metadata": {},
   "source": [
    "## Step 6: Lasso Regression (L1 Penalty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cb736dc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lasso Regression MSE: 0.04091897980722421\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Lasso Regression (with L1 penalty)\n",
    "lasso_model = Lasso(alpha=0.1)  # alpha is the lambda (regularization strength)\n",
    "lasso_model.fit(X_train, y_train)\n",
    "y_pred_lasso = lasso_model.predict(X_test)\n",
    "mse_lasso = mean_squared_error(y_test, y_pred_lasso)\n",
    "print(f\"Lasso Regression MSE: {mse_lasso}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "051ac732",
   "metadata": {},
   "source": [
    "## Step 7: Compare Coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6e4aea77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Coefficients Comparison:\n",
      "Linear Regression Coefficients: [-0.25113971  0.25971605  0.54009078]\n",
      "Ridge Regression Coefficients: [-0.20495223  0.21661856  0.51548391]\n",
      "Lasso Regression Coefficients: [0.         0.         0.37922386]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Compare coefficients\n",
    "print(\"\\nCoefficients Comparison:\")\n",
    "print(\"Linear Regression Coefficients:\", linear_model.coef_)\n",
    "print(\"Ridge Regression Coefficients:\", ridge_model.coef_)\n",
    "print(\"Lasso Regression Coefficients:\", lasso_model.coef_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f373b8fc",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "In this notebook, we compared Linear Regression, Ridge Regression, and Lasso Regression models on the Iris dataset. The MSE values and coefficients show how regularization techniques help improve model performance and feature selection.\n",
    "\n",
    "MSE tells us which model generalizes better to new data.\n",
    "\n",
    "Coefficients show the feature importance for each model and how regularization affects model complexity.\n",
    "\n",
    "Ridge vs. Lasso: If feature selection is desired, Lasso is often preferable; if smoother regularization is needed, Ridge is typically a better choice.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f9ab1008",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Regression MSE: 0.5305677824766752\n",
      "Linear Regression R²: 0.5957702326061664\n",
      "Ridge Regression Best Alpha: 0.001\n",
      "Ridge Regression MSE: 0.5305677196480341\n",
      "Ridge Regression R²: 0.5957702804741445\n",
      "Lasso Regression Best Alpha: 0.001\n",
      "Lasso Regression MSE: 0.5293486029376494\n",
      "Lasso Regression R²: 0.5966991029178375\n",
      "\n",
      "Coefficients Comparison:\n",
      "Linear Regression Coefficients: [ 4.45822565e-01  9.68186799e-03 -1.22095112e-01  7.78599557e-01\n",
      " -7.75740400e-07 -3.37002667e-03 -4.18536747e-01 -4.33687976e-01]\n",
      "Ridge Regression Coefficients: [ 4.45822383e-01  9.68187003e-03 -1.22094764e-01  7.78597623e-01\n",
      " -7.75735110e-07 -3.37002613e-03 -4.18536740e-01 -4.33687944e-01]\n",
      "Lasso Regression Coefficients: [ 4.41930348e-01  9.74218323e-03 -1.14609874e-01  7.37324328e-01\n",
      " -5.81572034e-07 -3.35392890e-03 -4.17269746e-01 -4.31811665e-01]\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import pandas as pd\n",
    "\n",
    "# Load the California Housing dataset\n",
    "data = fetch_california_housing()\n",
    "X = pd.DataFrame(data.data, columns=data.feature_names)  # Features\n",
    "y = pd.Series(data.target)  # Target: Median house value\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# 1. Linear Regression (Baseline)\n",
    "linear_model = LinearRegression()\n",
    "linear_model.fit(X_train, y_train)\n",
    "y_pred_linear = linear_model.predict(X_test)\n",
    "mse_linear = mean_squared_error(y_test, y_pred_linear)\n",
    "r2_linear = r2_score(y_test, y_pred_linear)\n",
    "print(f\"Linear Regression MSE: {mse_linear}\")\n",
    "print(f\"Linear Regression R²: {r2_linear}\")\n",
    "\n",
    "# Define a wider range of alpha values for Ridge and Lasso\n",
    "alpha_range = [0.001, 0.01, 0.1, 1, 10, 100, 1000]\n",
    "\n",
    "# 2. Ridge Regression with Grid Search\n",
    "ridge = Ridge()\n",
    "ridge_grid = GridSearchCV(ridge, param_grid={'alpha': alpha_range}, cv=5, scoring='neg_mean_squared_error')\n",
    "ridge_grid.fit(X_train, y_train)\n",
    "best_alpha_ridge = ridge_grid.best_params_['alpha']\n",
    "ridge_best_model = ridge_grid.best_estimator_\n",
    "y_pred_ridge = ridge_best_model.predict(X_test)\n",
    "mse_ridge = mean_squared_error(y_test, y_pred_ridge)\n",
    "r2_ridge = r2_score(y_test, y_pred_ridge)\n",
    "print(f\"Ridge Regression Best Alpha: {best_alpha_ridge}\")\n",
    "print(f\"Ridge Regression MSE: {mse_ridge}\")\n",
    "print(f\"Ridge Regression R²: {r2_ridge}\")\n",
    "\n",
    "# 3. Lasso Regression with Grid Search\n",
    "lasso = Lasso(max_iter=10000)  # Increase max_iter for Lasso if needed\n",
    "lasso_grid = GridSearchCV(lasso, param_grid={'alpha': alpha_range}, cv=5, scoring='neg_mean_squared_error')\n",
    "lasso_grid.fit(X_train, y_train)\n",
    "best_alpha_lasso = lasso_grid.best_params_['alpha']\n",
    "lasso_best_model = lasso_grid.best_estimator_\n",
    "y_pred_lasso = lasso_best_model.predict(X_test)\n",
    "mse_lasso = mean_squared_error(y_test, y_pred_lasso)\n",
    "r2_lasso = r2_score(y_test, y_pred_lasso)\n",
    "print(f\"Lasso Regression Best Alpha: {best_alpha_lasso}\")\n",
    "print(f\"Lasso Regression MSE: {mse_lasso}\")\n",
    "print(f\"Lasso Regression R²: {r2_lasso}\")\n",
    "\n",
    "# Compare coefficients\n",
    "print(\"\\nCoefficients Comparison:\")\n",
    "print(\"Linear Regression Coefficients:\", linear_model.coef_)\n",
    "print(\"Ridge Regression Coefficients:\", ridge_best_model.coef_)\n",
    "print(\"Lasso Regression Coefficients:\", lasso_best_model.coef_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "60ab7f90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Regression MSE: 0.5305677824766754\n",
      "Linear Regression R²: 0.5957702326061662\n",
      "Ridge Regression Best Alpha: 1\n",
      "Ridge Regression MSE: 0.5305421966451027\n",
      "Ridge Regression R²: 0.5957897259773939\n",
      "Lasso Regression Best Alpha: 0.001\n",
      "Lasso Regression MSE: 0.5297114575950318\n",
      "Lasso Regression R²: 0.5964226506744186\n",
      "\n",
      "Coefficients Comparison:\n",
      "Linear Regression Coefficients: [ 8.46962874e-01  1.21848755e-01 -3.02077128e-01  3.68977844e-01\n",
      " -8.78475338e-04 -3.50004161e-02 -8.93952910e-01 -8.68886568e-01]\n",
      "Ridge Regression Coefficients: [ 8.46884862e-01  1.21938029e-01 -3.01803727e-01  3.68629608e-01\n",
      " -8.48229744e-04 -3.50046562e-02 -8.93104816e-01 -8.68024976e-01]\n",
      "Lasso Regression Coefficients: [ 0.84148101  0.12254712 -0.28779358  0.35385692 -0.         -0.0343095\n",
      " -0.88327023 -0.85748112]\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "\n",
    "# Load the California Housing dataset\n",
    "data = fetch_california_housing()\n",
    "X = pd.DataFrame(data.data, columns=data.feature_names)  # Features\n",
    "y = pd.Series(data.target)  # Target: Median house value\n",
    "\n",
    "# Handle missing values (if any) by dropping rows with NaNs\n",
    "X.dropna(inplace=True)\n",
    "y = y[X.index]  # Keep target aligned with features after dropping NaNs\n",
    "\n",
    "# Feature normalization\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)  # Fit and transform X\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# 1. Linear Regression (Baseline)\n",
    "linear_model = LinearRegression()\n",
    "linear_model.fit(X_train, y_train)\n",
    "y_pred_linear = linear_model.predict(X_test)\n",
    "mse_linear = mean_squared_error(y_test, y_pred_linear)\n",
    "r2_linear = r2_score(y_test, y_pred_linear)\n",
    "print(f\"Linear Regression MSE: {mse_linear}\")\n",
    "print(f\"Linear Regression R²: {r2_linear}\")\n",
    "\n",
    "# Define a wider range of alpha values for Ridge and Lasso\n",
    "alpha_range = [0.001, 0.01, 0.1, 1, 10, 100, 1000]\n",
    "\n",
    "# 2. Ridge Regression with Grid Search\n",
    "ridge = Ridge()\n",
    "ridge_grid = GridSearchCV(ridge, param_grid={'alpha': alpha_range}, cv=5, scoring='neg_mean_squared_error')\n",
    "ridge_grid.fit(X_train, y_train)\n",
    "best_alpha_ridge = ridge_grid.best_params_['alpha']\n",
    "ridge_best_model = ridge_grid.best_estimator_\n",
    "y_pred_ridge = ridge_best_model.predict(X_test)\n",
    "mse_ridge = mean_squared_error(y_test, y_pred_ridge)\n",
    "r2_ridge = r2_score(y_test, y_pred_ridge)\n",
    "print(f\"Ridge Regression Best Alpha: {best_alpha_ridge}\")\n",
    "print(f\"Ridge Regression MSE: {mse_ridge}\")\n",
    "print(f\"Ridge Regression R²: {r2_ridge}\")\n",
    "\n",
    "# 3. Lasso Regression with Grid Search\n",
    "lasso = Lasso(max_iter=10000)  # Increase max_iter for Lasso if needed\n",
    "lasso_grid = GridSearchCV(lasso, param_grid={'alpha': alpha_range}, cv=5, scoring='neg_mean_squared_error')\n",
    "lasso_grid.fit(X_train, y_train)\n",
    "best_alpha_lasso = lasso_grid.best_params_['alpha']\n",
    "lasso_best_model = lasso_grid.best_estimator_\n",
    "y_pred_lasso = lasso_best_model.predict(X_test)\n",
    "mse_lasso = mean_squared_error(y_test, y_pred_lasso)\n",
    "r2_lasso = r2_score(y_test, y_pred_lasso)\n",
    "print(f\"Lasso Regression Best Alpha: {best_alpha_lasso}\")\n",
    "print(f\"Lasso Regression MSE: {mse_lasso}\")\n",
    "print(f\"Lasso Regression R²: {r2_lasso}\")\n",
    "\n",
    "# Compare coefficients\n",
    "print(\"\\nCoefficients Comparison:\")\n",
    "print(\"Linear Regression Coefficients:\", linear_model.coef_)\n",
    "print(\"Ridge Regression Coefficients:\", ridge_best_model.coef_)\n",
    "print(\"Lasso Regression Coefficients:\", lasso_best_model.coef_)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
